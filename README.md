Welcome to the GitHub repository for the paper titled "Towards a performance analysis on pre-trained Visual Question Answering models for autonomous driving." 

The supplementary material provided in this GitHub repository includes three key components: a comprehensive corpus comprising 78 research papers focused on Visual Question Answering (VQA) models, a test dataset used for evaluating three specific models (ViLBERT, ViLT, and LXMERT), and an in-depth analysis of the results obtained from the evaluation of these models in the context of autonomous driving. 
Researchers and practitioners can leverage these resources to access a wide range of relevant literature, utilize the test dataset for benchmarking purposes, and delve into the detailed analysis to gain insights into the performance of VQA models for autonomous driving applications.
